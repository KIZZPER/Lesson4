# Результаты экспериментов со сверточными сетями


## Задание 1: Сравнение CNN и полносвязных сетей (FC)

### Сравнение на MNIST

*   **Полносвязная сеть**: Показала неплохой результат, но потребовала много параметров.
*   **Простая CNN**: Оказалась значительно эффективнее FC. Точность выше, а параметров меньше.
*   **CNN с Residual блоками**: Демонстрирует самую высокую точность, доказывая пользу остаточных связей даже на простой задаче.

**Вывод**: CNN превосходят полносвязные сети для задач с изображениями, так как они лучше улавливают пространственные признаки (линии, углы).

### Сравнение на CIFAR-10

*   **Полносвязная сеть**: С трудом справилась с более сложными цветными изображениями CIFAR-10. Точность оказалась очень низкой.
*   **CNN с Residual блоками**: Показала на порядок лучший результат, легко обогнав FC.
*   **CNN с регуляризацией (Dropout)**: Добавление Dropout помогло уменьшить переобучение** и немного улучшило итоговую точность на тестовых данных.

**Вывод**: На сложных наборах данных, таких как CIFAR-10, преимущество CNN над FC становится неоспоримым. Регуляризация является важным инструментом для борьбы с переобучением.

## Задание 2: Анализ архитектур CNN

### Влияние размера ядра свертки

*   **Ядро 3x3**: Обеспечило лучший баланс между точностью, скоростью обучения и количеством параметров.
*   **Ядра 5x5 и 7x7**: Увеличили количество параметров и замедлили обучение, не дав значительного прироста в точности.

**Вывод**: Ядра 3x3 более эффективны. Глубокая сеть из маленьких ядер почти всегда лучше, чем неглубокая сеть с большими ядрами.

### Влияние глубины сети

*   **Неглубокие CNN (2-4 слоя)**: Быстро обучаются, но их точность ограничена.
*   **Глубокая CNN (6+ слоев)**: Увеличение глубины без остаточных связей привело к проблеме затухания градиента. Точность перестала расти или даже начала падать.
*   **CNN с Residual связями**: Показала лучший результат. Остаточные связи позволили эффективно обучать глубокую сеть, и точность стабильно росла.

**Вывод**: Глубина - ключ к высокой точности, но только при использовании специальных техник, таких как Residual-блоки.

## Задание 3: Кастомные слои и эксперименты

### Кастомные слои

*   **Depthwise Separable Convolution**: Значительно сократила количество параметров и ускорила модель при небольшом падении точности. Отличный выбор для мобильных устройств.
*   **Активация Mish**: Показала себя немного лучше стандартной ReLU, обеспечив более стабильное обучение.

### Эксперименты с Residual блоками

*   **Bottleneck блок**: Позволил уменьшить число параметров в глубоких сетях, что ускоряет обучение.
*   **Wide Residual блок**: Увеличение ширины вместо глубины также оказалось эффективной стратегией для повышения точности.
*   **Блок с Attention**: Добавление механизма внимания дало небольшой прирост точности, позволив модели фокусироваться на самых важных частях изображения.

## Итог
 Архитектура нейронной сети имеет огромное значение. CNN фундаментально лучше подходят для изображений, чем FC. Внутри CNN такие элементы, как **размер ядра**, **глубина**, **Residual-блоки** и **механизмы внимания**, являются мощными инструментами для достижения баланса между точностью, скоростью и размером модели.
